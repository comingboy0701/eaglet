# coding: UTF-8import osimport torchimport torch.nn as nnfrom transformers import BertTokenizerfrom script.classifier.nn_classifier.classifier import Classifierfrom script.classifier.nn_classifier.text_utils import BertDatasetfrom script.classifier.setting import DataConfigimport torch.nn.functional as Ffrom importlib import import_modulefrom .optimization import BertAdamimport sysclass Config(DataConfig):    """数据配置参数"""    def __init__(self, **kwargs):        DataConfig.__init__(self,                            dataset=kwargs.get('dataset'),                            cache_dir=kwargs.get('cache_dir'),                            model_name=kwargs.get('model_name'),                            adversarial=kwargs.get('adversarial'),)        self.require_improvement = 1000  # 若超过1000batch效果还没提升，则提前结束训练        self.num_epochs = 3  # epoch数        self.batch_size = 128  # mini-batch大小        self.pad_size = 32  # 每句话处理成的长度(短填长切)        self.learning_rate = 5e-5  # 学习率        self.hidden_size = 768        self.bert_type = "bert"        self.bert_path = 'bert_model/NN/bert-base-chinese'        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)        # self.bert_type = "nezha"        # self.bert_path = 'bert_model/NN/nezha-chinese-base'        # self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)class Model(nn.Module):    def __init__(self, config):        super(Model, self).__init__()        self.config = config        if config.bert_type == "bert":            from transformers import BertModel            self.bert = BertModel.from_pretrained(config.bert_path)        elif config.bert_type == "nezha":            from script.model.NEZHA.modeling_nezha import BertModel            self.bert = BertModel.from_pretrained(config.bert_path)        else:             sys.exit("config 必须提供 bert_type")        for param in self.bert.parameters():            param.requires_grad = True        self.fc = nn.Linear(config.hidden_size, config.num_classes)    def forward(self, x, labels=None):        context = x[0]  # 输入的句子        mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]        if self.config.bert_type == "bert":            _, pooled = self.bert(context, attention_mask=mask, output_hidden_states=False, return_dict=False)        elif self.config.bert_type == "nezha":            _, pooled = self.bert(context, attention_mask=mask, output_all_encoded_layers=False)        else:             sys.exit("config 必须提供 bert_type")        out = self.fc(pooled)        if labels is not None:            # loss_mask = labels.gt(-1)            loss = F.cross_entropy(out, labels)            return out, loss        return outclass BertClassifier(Classifier, Config):    def __init__(self, **kwargs):        Config.__init__(self,                        dataset=kwargs.get('dataset'),                        cache_dir=kwargs.get('cache_dir'),                        model_name=kwargs.get('model_name'),                        adversarial=kwargs.get('adversarial')                        )        self.model = Model(self).to(self.device)        # self.logger.info(self.model.parameters)        adversarial = kwargs.get('adversarial', 'base')        at_model = import_module("script.adversarial.{}_model".format(adversarial))        self.atModel = at_model.ATModel(self.model)        self.modelDataset = BertDataset        self._set_optimizer()        self._set_loss_fun()        if kwargs.get('evaluate') and os.path.exists(self.save_model_path):            self.logger.info("load model_path: {}".format(self.save_model_path))            self.model.load_state_dict(torch.load(self.save_model_path))            self.model.eval()            self.logger.info("load {} success".format(self.model_name))    def _set_optimizer(self, ):        # param_optimizer = list(self.model.named_parameters())        # no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']        # optimizer_grouped_parameters = [        #     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},        #     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]        # train_iter = self._covert_sentence_label(self.train_path, self.toy)        # self.optimizer = BertAdam(optimizer_grouped_parameters,        #                           lr=self.learning_rate,        #                           warmup=0.05,        #                           t_total=len(train_iter) * self.num_epochs)        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)    def _set_loss_fun(self, ):        self.loss_fun = F.cross_entropy